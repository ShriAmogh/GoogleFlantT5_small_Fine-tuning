{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12106222,"sourceType":"datasetVersion","datasetId":7621818},{"sourceId":12106230,"sourceType":"datasetVersion","datasetId":7621826},{"sourceId":12106283,"sourceType":"datasetVersion","datasetId":7621867},{"sourceId":12106289,"sourceType":"datasetVersion","datasetId":7621872}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9e9c22c8-ad35-4bf4-8bf0-8cc8ca909977","cell_type":"code","source":"!pip install bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:42:59.179188Z","iopub.execute_input":"2025-06-12T09:42:59.179540Z","iopub.status.idle":"2025-06-12T09:44:18.669831Z","shell.execute_reply.started":"2025-06-12T09:42:59.179511Z","shell.execute_reply":"2025-06-12T09:44:18.668907Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"id":"9da73dab","cell_type":"code","source":"#creating new csv \nimport pandas as pd\n\ndf = pd.read_csv('/kaggle/input/csv-gm8k/gm8k.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:44:18.674230Z","iopub.execute_input":"2025-06-12T09:44:18.674478Z","iopub.status.idle":"2025-06-12T09:44:19.000451Z","shell.execute_reply.started":"2025-06-12T09:44:18.674452Z","shell.execute_reply":"2025-06-12T09:44:18.999796Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Janet’s ducks lay 16 eggs per day. She eats th...   \n1  A robe takes 2 bolts of blue fiber and half th...   \n2  Josh decides to try flipping a house.  He buys...   \n3  James decides to run 3 sprints 3 times a week....   \n4  Every day, Wendi feeds each of her chickens th...   \n\n                                              answer final_solution  \n0  Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eg...             18  \n1  It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nS...              3  \n2  The cost of the house and repairs came out to ...          70000  \n3  He sprints 3*3=<<3*3=9>>9 times\\nSo he runs 9*...            540  \n4  If each chicken eats 3 cups of feed per day, t...             20  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>final_solution</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Janet’s ducks lay 16 eggs per day. She eats th...</td>\n      <td>Janet sells 16 - 3 - 4 = &lt;&lt;16-3-4=9&gt;&gt;9 duck eg...</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A robe takes 2 bolts of blue fiber and half th...</td>\n      <td>It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber\\nS...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Josh decides to try flipping a house.  He buys...</td>\n      <td>The cost of the house and repairs came out to ...</td>\n      <td>70000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>James decides to run 3 sprints 3 times a week....</td>\n      <td>He sprints 3*3=&lt;&lt;3*3=9&gt;&gt;9 times\\nSo he runs 9*...</td>\n      <td>540</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Every day, Wendi feeds each of her chickens th...</td>\n      <td>If each chicken eats 3 cups of feed per day, t...</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"id":"1bddaa3e","cell_type":"code","source":"#df.drop('final_solution',axis= 1 , inplace= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:50:43.474745Z","iopub.execute_input":"2025-06-10T18:50:43.474946Z","iopub.status.idle":"2025-06-10T18:50:43.485100Z","shell.execute_reply.started":"2025-06-10T18:50:43.474930Z","shell.execute_reply":"2025-06-10T18:50:43.484299Z"}},"outputs":[],"execution_count":3},{"id":"2855b6f3","cell_type":"code","source":"#df.head()\n#df.to_csv('gm8k_ans.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:50:43.486648Z","iopub.execute_input":"2025-06-10T18:50:43.487056Z","iopub.status.idle":"2025-06-10T18:50:43.520809Z","shell.execute_reply.started":"2025-06-10T18:50:43.487038Z","shell.execute_reply":"2025-06-10T18:50:43.520242Z"}},"outputs":[],"execution_count":4},{"id":"0a00ecea","cell_type":"code","source":"'''df = pd.read_csv('G:/Python/fine_tune_improve_mathematical_reasoning/Qwen2.5-1.5B_Fine-tuning/dataset/gm8k_new.csv')\ndf.head()'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:50:43.521405Z","iopub.execute_input":"2025-06-10T18:50:43.521588Z","iopub.status.idle":"2025-06-10T18:50:43.525836Z","shell.execute_reply.started":"2025-06-10T18:50:43.521573Z","shell.execute_reply":"2025-06-10T18:50:43.525320Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"\"df = pd.read_csv('G:/Python/fine_tune_improve_mathematical_reasoning/Qwen2.5-1.5B_Fine-tuning/dataset/gm8k_new.csv')\\ndf.head()\""},"metadata":{}}],"execution_count":5},{"id":"9c2561db","cell_type":"code","source":"from datasets import Dataset\ndataset = Dataset.from_pandas(df[[\"question\", \"answer\", \"final_solution\"]])\ndataset = dataset.train_test_split(test_size= 0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:45:04.639755Z","iopub.execute_input":"2025-06-12T09:45:04.640067Z","iopub.status.idle":"2025-06-12T09:45:06.158254Z","shell.execute_reply.started":"2025-06-12T09:45:04.640044Z","shell.execute_reply":"2025-06-12T09:45:06.157502Z"}},"outputs":[],"execution_count":3},{"id":"5c61500d-d706-436f-b1de-7585f1d14bc2","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport bitsandbytes as bnb  \n\nmodel_id = \"Qwen/Qwen2.5-1.5B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_8bit=True,           \n    device_map=\"cuda\",         \n    trust_remote_code=True,\n)\n#model = model.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:48:02.716584Z","iopub.execute_input":"2025-06-12T09:48:02.717332Z","iopub.status.idle":"2025-06-12T09:48:06.048899Z","shell.execute_reply.started":"2025-06-12T09:48:02.717310Z","shell.execute_reply":"2025-06-12T09:48:06.048170Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"}],"execution_count":5},{"id":"c4281f3f","cell_type":"code","source":"def preprocess_qwen(example):\n    full_prompt = f\"<|user|>\\nSolve the following math problem step-by-step:\\n{example['question']}\\n<|assistant|>\\n{example['answer']}\"\n    encoded = tokenizer(full_prompt, truncation=True, padding=\"max_length\", max_length=1024)\n    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n    encoded[\"question\"] = example[\"question\"]\n    encoded[\"answer\"] = example[\"answer\"]\n    encoded[\"final_solution\"] = example[\"final_solution\"]\n    return encoded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:57:55.174352Z","iopub.execute_input":"2025-06-12T09:57:55.175174Z","iopub.status.idle":"2025-06-12T09:57:55.181343Z","shell.execute_reply.started":"2025-06-12T09:57:55.175141Z","shell.execute_reply":"2025-06-12T09:57:55.180480Z"}},"outputs":[],"execution_count":9},{"id":"89dad912-4a8a-4457-bafe-72d22ee80770","cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_qwen, batched=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T10:00:58.814273Z","iopub.execute_input":"2025-06-12T10:00:58.814899Z","iopub.status.idle":"2025-06-12T10:01:00.809716Z","shell.execute_reply.started":"2025-06-12T10:00:58.814871Z","shell.execute_reply":"2025-06-12T10:01:00.808524Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1055 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94ee317a9cb4000bdedd49712f364d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/264 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c64facfedef42a0a1f34b850d9c327b"}},"metadata":{}}],"execution_count":11},{"id":"5cb658c4","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\nmodel_id = \"Qwen/Qwen2.5-1.5B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_8bit=True,\n    device_map=\"auto\",     \n    trust_remote_code=True,\n    torch_dtype=torch.float16   \n)\n\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_config = LoraConfig(\n    r=16,                          \n    lora_alpha=32,               \n    target_modules=[\"q_proj\", \"v_proj\"], \n    lora_dropout=0.1,                  \n    bias=\"none\",                     \n    task_type=\"CAUSAL_LM\"    \n)\n\nmodel = get_peft_model(model, lora_config)\n\nmodel.print_trainable_parameters()\n\nprint(f\"Model on device: {next(model.parameters()).device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:54:40.879408Z","iopub.execute_input":"2025-06-12T09:54:40.880160Z","iopub.status.idle":"2025-06-12T09:54:45.223089Z","shell.execute_reply.started":"2025-06-12T09:54:40.880127Z","shell.execute_reply":"2025-06-12T09:54:45.222290Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\nModel on device: cuda:0\n","output_type":"stream"}],"execution_count":7},{"id":"96ac2bb2","cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen-lora-finetuned\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=3,\n    fp16=True, \n    logging_steps=50,\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\", \n    learning_rate=2e-4,\n    warmup_steps=100,\n    optim=\"paged_adamw_8bit\",\n    label_names=[\"labels\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T10:01:05.566279Z","iopub.execute_input":"2025-06-12T10:01:05.566556Z","iopub.status.idle":"2025-06-12T10:01:05.615235Z","shell.execute_reply.started":"2025-06-12T10:01:05.566536Z","shell.execute_reply":"2025-06-12T10:01:05.614690Z"}},"outputs":[],"execution_count":12},{"id":"a283954f","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T10:01:11.526416Z","iopub.execute_input":"2025-06-12T10:01:11.526690Z","iopub.status.idle":"2025-06-12T11:12:52.777793Z","shell.execute_reply.started":"2025-06-12T10:01:11.526670Z","shell.execute_reply":"2025-06-12T11:12:52.777157Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [198/198 1:11:17, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.253300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.884100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.784300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=198, training_loss=0.9277083079020182, metrics={'train_runtime': 4300.778, 'train_samples_per_second': 0.736, 'train_steps_per_second': 0.046, 'total_flos': 2.55229426925568e+16, 'train_loss': 0.9277083079020182, 'epoch': 3.0})"},"metadata":{}}],"execution_count":13},{"id":"3be943df-cf4c-46e5-8007-ce619a09cffe","cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/qwen_new-lora-adapters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T11:12:53.339879Z","iopub.execute_input":"2025-06-12T11:12:53.340224Z","iopub.status.idle":"2025-06-12T11:12:53.458150Z","shell.execute_reply.started":"2025-06-12T11:12:53.340195Z","shell.execute_reply":"2025-06-12T11:12:53.457298Z"}},"outputs":[],"execution_count":16},{"id":"65bb0b02-36f2-4dd2-bd69-fb3a63e65e08","cell_type":"code","source":"tokenizer.save_pretrained(\"/kaggle/working/qwen_new-lora-adapters\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T11:12:53.885385Z","iopub.execute_input":"2025-06-12T11:12:53.885659Z","iopub.status.idle":"2025-06-12T11:12:54.088090Z","shell.execute_reply.started":"2025-06-12T11:12:53.885636Z","shell.execute_reply":"2025-06-12T11:12:54.087231Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/qwen_new-lora-adapters/tokenizer_config.json',\n '/kaggle/working/qwen_new-lora-adapters/special_tokens_map.json',\n '/kaggle/working/qwen_new-lora-adapters/vocab.json',\n '/kaggle/working/qwen_new-lora-adapters/merges.txt',\n '/kaggle/working/qwen_new-lora-adapters/added_tokens.json',\n '/kaggle/working/qwen_new-lora-adapters/tokenizer.json')"},"metadata":{}}],"execution_count":19},{"id":"0cd75da2-161a-42e0-ba89-0f6efe2100e3","cell_type":"code","source":"import os\n\noutput_path = \"/kaggle/working/qwen_new-lora-adapters\"\nprint(\"Exists:\", os.path.exists(output_path))\nprint(\"Contents:\", os.listdir(output_path) if os.path.exists(output_path) else \"Not found\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T11:44:43.510825Z","iopub.execute_input":"2025-06-12T11:44:43.511570Z","iopub.status.idle":"2025-06-12T11:44:43.516042Z","shell.execute_reply.started":"2025-06-12T11:44:43.511545Z","shell.execute_reply":"2025-06-12T11:44:43.515164Z"}},"outputs":[{"name":"stdout","text":"Exists: True\nContents: ['added_tokens.json', 'special_tokens_map.json', 'adapter_model.safetensors', 'tokenizer.json', 'adapter_config.json', 'vocab.json', 'tokenizer_config.json', 'merges.txt', 'README.md']\n","output_type":"stream"}],"execution_count":21},{"id":"0f4e8d5b-2c5a-4298-8cfa-84774d372537","cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/qwen_new-lora-adapters\", 'zip', \"/kaggle/working/qwen_new-lora-adapters\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T11:44:59.254309Z","iopub.execute_input":"2025-06-12T11:44:59.254828Z","iopub.status.idle":"2025-06-12T11:45:00.396589Z","shell.execute_reply.started":"2025-06-12T11:44:59.254807Z","shell.execute_reply":"2025-06-12T11:45:00.395971Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/qwen_new-lora-adapters.zip'"},"metadata":{}}],"execution_count":22},{"id":"2d376dde-1e37-4790-9b68-7f1acecb2508","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}