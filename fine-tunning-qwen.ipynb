{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12106222,"sourceType":"datasetVersion","datasetId":7621818},{"sourceId":12106230,"sourceType":"datasetVersion","datasetId":7621826},{"sourceId":12106283,"sourceType":"datasetVersion","datasetId":7621867},{"sourceId":12106289,"sourceType":"datasetVersion","datasetId":7621872}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9e9c22c8-ad35-4bf4-8bf0-8cc8ca909977","cell_type":"code","source":"!pip install bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:49:24.243644Z","iopub.execute_input":"2025-06-10T18:49:24.244139Z","iopub.status.idle":"2025-06-10T18:50:43.150184Z","shell.execute_reply.started":"2025-06-10T18:49:24.244115Z","shell.execute_reply":"2025-06-10T18:50:43.149345Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"id":"9da73dab","cell_type":"code","source":"#creating new csv \nimport pandas as pd\n\ndf = pd.read_csv('/kaggle/input/csv-gm8k/gm8k.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:50:43.151759Z","iopub.execute_input":"2025-06-10T18:50:43.152338Z","iopub.status.idle":"2025-06-10T18:50:43.474034Z","shell.execute_reply.started":"2025-06-10T18:50:43.152308Z","shell.execute_reply":"2025-06-10T18:50:43.473319Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Janet’s ducks lay 16 eggs per day. She eats th...   \n1  A robe takes 2 bolts of blue fiber and half th...   \n2  Josh decides to try flipping a house.  He buys...   \n3  James decides to run 3 sprints 3 times a week....   \n4  Every day, Wendi feeds each of her chickens th...   \n\n                                              answer final_solution  \n0  Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eg...             18  \n1  It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nS...              3  \n2  The cost of the house and repairs came out to ...          70000  \n3  He sprints 3*3=<<3*3=9>>9 times\\nSo he runs 9*...            540  \n4  If each chicken eats 3 cups of feed per day, t...             20  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n      <th>final_solution</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Janet’s ducks lay 16 eggs per day. She eats th...</td>\n      <td>Janet sells 16 - 3 - 4 = &lt;&lt;16-3-4=9&gt;&gt;9 duck eg...</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A robe takes 2 bolts of blue fiber and half th...</td>\n      <td>It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber\\nS...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Josh decides to try flipping a house.  He buys...</td>\n      <td>The cost of the house and repairs came out to ...</td>\n      <td>70000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>James decides to run 3 sprints 3 times a week....</td>\n      <td>He sprints 3*3=&lt;&lt;3*3=9&gt;&gt;9 times\\nSo he runs 9*...</td>\n      <td>540</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Every day, Wendi feeds each of her chickens th...</td>\n      <td>If each chicken eats 3 cups of feed per day, t...</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"id":"1bddaa3e","cell_type":"code","source":"df.drop('final_solution',axis= 1 , inplace= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:50:43.474745Z","iopub.execute_input":"2025-06-10T18:50:43.474946Z","iopub.status.idle":"2025-06-10T18:50:43.485100Z","shell.execute_reply.started":"2025-06-10T18:50:43.474930Z","shell.execute_reply":"2025-06-10T18:50:43.484299Z"}},"outputs":[],"execution_count":3},{"id":"2855b6f3","cell_type":"code","source":"df.head()\ndf.to_csv('gm8k_ans.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:50:43.486648Z","iopub.execute_input":"2025-06-10T18:50:43.487056Z","iopub.status.idle":"2025-06-10T18:50:43.520809Z","shell.execute_reply.started":"2025-06-10T18:50:43.487038Z","shell.execute_reply":"2025-06-10T18:50:43.520242Z"}},"outputs":[],"execution_count":4},{"id":"0a00ecea","cell_type":"code","source":"'''df = pd.read_csv('G:/Python/fine_tune_improve_mathematical_reasoning/Qwen2.5-1.5B_Fine-tuning/dataset/gm8k_new.csv')\ndf.head()'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:50:43.521405Z","iopub.execute_input":"2025-06-10T18:50:43.521588Z","iopub.status.idle":"2025-06-10T18:50:43.525836Z","shell.execute_reply.started":"2025-06-10T18:50:43.521573Z","shell.execute_reply":"2025-06-10T18:50:43.525320Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"\"df = pd.read_csv('G:/Python/fine_tune_improve_mathematical_reasoning/Qwen2.5-1.5B_Fine-tuning/dataset/gm8k_new.csv')\\ndf.head()\""},"metadata":{}}],"execution_count":5},{"id":"9c2561db","cell_type":"code","source":"from datasets import Dataset\ndataset = Dataset.from_pandas(df[[\"question\", \"answer\"]])\ndataset = dataset.train_test_split(test_size= 0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:51:11.096751Z","iopub.execute_input":"2025-06-10T18:51:11.097019Z","iopub.status.idle":"2025-06-10T18:51:12.816586Z","shell.execute_reply.started":"2025-06-10T18:51:11.096999Z","shell.execute_reply":"2025-06-10T18:51:12.815822Z"}},"outputs":[],"execution_count":6},{"id":"5c61500d-d706-436f-b1de-7585f1d14bc2","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport bitsandbytes as bnb  \n\nmodel_id = \"Qwen/Qwen2.5-1.5B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_8bit=True,           \n    device_map=\"auto\",         \n    trust_remote_code=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:51:30.230910Z","iopub.execute_input":"2025-06-10T18:51:30.231545Z","iopub.status.idle":"2025-06-10T18:52:14.065645Z","shell.execute_reply.started":"2025-06-10T18:51:30.231519Z","shell.execute_reply":"2025-06-10T18:52:14.065028Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f4eb095e8843f4a815547f895d7a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18661dd1edb4474a816851d202527be3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb463aac2144d509d42bb1dc74ec311"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f152c91b684d1ebc86d9e6294e04c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54622b3ada9041ee95fadd14bb80386f"}},"metadata":{}},{"name":"stderr","text":"2025-06-10 18:51:45.010084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749581505.194863      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749581505.247577      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7210c67efbe444d8089b1ee096a5aed"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83adea52bb774ec8b3e87f39ef0554ef"}},"metadata":{}}],"execution_count":7},{"id":"c4281f3f","cell_type":"code","source":"def preprocess_qwen(example):\n    full_prompt = f\"<|user|>\\nQuestion: {example['question']}\\n<|assistant|>\\n{example['answer']}\"\n    encoded = tokenizer(full_prompt, truncation=True, padding=\"max_length\", max_length=1024)\n    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n    return encoded\n\ntokenized_dataset = dataset.map(preprocess_qwen, batched=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:52:18.041087Z","iopub.execute_input":"2025-06-10T18:52:18.042458Z","iopub.status.idle":"2025-06-10T18:52:20.071425Z","shell.execute_reply.started":"2025-06-10T18:52:18.042400Z","shell.execute_reply":"2025-06-10T18:52:20.070446Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1055 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ec95b58e0e49a789fedfe4ac4c645f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/264 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9598b8726c7a4a2482cb82bd3af7f6c7"}},"metadata":{}}],"execution_count":8},{"id":"5cb658c4","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\n\nmodel_id = \"Qwen/Qwen2.5-1.5B\"\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_8bit=True,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16\n)\n\n\nmodel = prepare_model_for_kbit_training(model)\n\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"], \n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, lora_config)\n\n\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T18:57:40.933051Z","iopub.execute_input":"2025-06-10T18:57:40.933636Z","iopub.status.idle":"2025-06-10T18:57:44.571155Z","shell.execute_reply.started":"2025-06-10T18:57:40.933612Z","shell.execute_reply":"2025-06-10T18:57:44.570594Z"}},"outputs":[{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,179,072 || all params: 1,545,893,376 || trainable%: 0.1410\n","output_type":"stream"}],"execution_count":16},{"id":"96ac2bb2","cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen-lora-finetuned\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=3,\n    fp16=True,  # mixed-precision training\n    logging_steps=50,\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\",  # disable wandb/huggingface logging\n    learning_rate=2e-4,\n    warmup_steps=100,\n    optim=\"paged_adamw_8bit\",  # recommended for 8-bit + LoRA\n    label_names=[\"labels\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator,\n\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T19:01:44.097700Z","iopub.execute_input":"2025-06-10T19:01:44.098323Z","iopub.status.idle":"2025-06-10T19:01:44.139194Z","shell.execute_reply.started":"2025-06-10T19:01:44.098299Z","shell.execute_reply":"2025-06-10T19:01:44.138689Z"}},"outputs":[],"execution_count":19},{"id":"a283954f","cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T19:01:50.369351Z","iopub.execute_input":"2025-06-10T19:01:50.369637Z","iopub.status.idle":"2025-06-10T20:10:29.054657Z","shell.execute_reply.started":"2025-06-10T19:01:50.369619Z","shell.execute_reply":"2025-06-10T20:10:29.054034Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [198/198 1:08:15, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.188500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.888800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.823300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=198, training_loss=0.9284252975926255, metrics={'train_runtime': 4118.2378, 'train_samples_per_second': 0.769, 'train_steps_per_second': 0.048, 'total_flos': 2.55229426925568e+16, 'train_loss': 0.9284252975926255, 'epoch': 3.0})"},"metadata":{}}],"execution_count":20},{"id":"3be943df-cf4c-46e5-8007-ce619a09cffe","cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/qwen-lora-adapters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:12:00.131072Z","iopub.execute_input":"2025-06-10T20:12:00.131580Z","iopub.status.idle":"2025-06-10T20:12:00.253570Z","shell.execute_reply.started":"2025-06-10T20:12:00.131557Z","shell.execute_reply":"2025-06-10T20:12:00.252836Z"}},"outputs":[],"execution_count":25},{"id":"65bb0b02-36f2-4dd2-bd69-fb3a63e65e08","cell_type":"code","source":"tokenizer.save_pretrained(\"/kaggle/working/qwen-lora-adapters\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:12:02.906374Z","iopub.execute_input":"2025-06-10T20:12:02.906682Z","iopub.status.idle":"2025-06-10T20:12:03.104639Z","shell.execute_reply.started":"2025-06-10T20:12:02.906662Z","shell.execute_reply":"2025-06-10T20:12:03.103798Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/qwen-lora-adapters/tokenizer_config.json',\n '/kaggle/working/qwen-lora-adapters/special_tokens_map.json',\n '/kaggle/working/qwen-lora-adapters/vocab.json',\n '/kaggle/working/qwen-lora-adapters/merges.txt',\n '/kaggle/working/qwen-lora-adapters/added_tokens.json',\n '/kaggle/working/qwen-lora-adapters/tokenizer.json')"},"metadata":{}}],"execution_count":26},{"id":"0cd75da2-161a-42e0-ba89-0f6efe2100e3","cell_type":"code","source":"import os\n\noutput_path = \"/kaggle/working/qwen-lora-adapters\"\nprint(\"Exists:\", os.path.exists(output_path))\nprint(\"Contents:\", os.listdir(output_path) if os.path.exists(output_path) else \"Not found\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:14:01.751082Z","iopub.execute_input":"2025-06-10T20:14:01.751699Z","iopub.status.idle":"2025-06-10T20:14:01.756503Z","shell.execute_reply.started":"2025-06-10T20:14:01.751676Z","shell.execute_reply":"2025-06-10T20:14:01.755658Z"}},"outputs":[{"name":"stdout","text":"Exists: True\nContents: ['added_tokens.json', 'special_tokens_map.json', 'tokenizer_config.json', 'README.md', 'tokenizer.json', 'adapter_model.safetensors', 'adapter_config.json', 'merges.txt', 'vocab.json']\n","output_type":"stream"}],"execution_count":27},{"id":"0f4e8d5b-2c5a-4298-8cfa-84774d372537","cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/qwen-lora-adapters\", 'zip', \"/kaggle/working/qwen-lora-adapters\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T20:15:07.868182Z","iopub.execute_input":"2025-06-10T20:15:07.868861Z","iopub.status.idle":"2025-06-10T20:15:09.016897Z","shell.execute_reply.started":"2025-06-10T20:15:07.868838Z","shell.execute_reply":"2025-06-10T20:15:09.016213Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/qwen-lora-adapters.zip'"},"metadata":{}}],"execution_count":28},{"id":"2d376dde-1e37-4790-9b68-7f1acecb2508","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}