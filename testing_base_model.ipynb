{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7696f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f90f1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  \\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name,  \\n                                            torch_dtype= \"float16\")\\nmodel.eval()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,  \n",
    "                                            torch_dtype= \"float16\")\n",
    "model.eval()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe3e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "'''#quantization\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True,        #or load_in_4bit=True for more compression\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    #llm_int8_enable_fp32_cpu_offload=True\n",
    ")'''\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "                                                   #quantization_config = bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279c8243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993eb13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 'Level 4',\n",
      " 'problem': 'We flip a fair coin 10 times.  What is the probability that we '\n",
      "            'get heads in at least 6 of the 10 flips?',\n",
      " 'solution': 'There are $2^{10} = 1024$ possible outcomes of the 10 coin '\n",
      "             'flips. The probability that we flip at least 6 heads is equal to '\n",
      "             'the probability that we flip at least 6 tails, by symmetry.  '\n",
      "             \"Let's call this probability $p$.  The only other possibility is \"\n",
      "             'that we flip exactly 5 heads and 5 tails, for which the '\n",
      "             'probability is $\\\\dfrac{\\\\binom{10}{5}}{2^{10}} = '\n",
      "             '\\\\dfrac{252}{1024} = \\\\dfrac{63}{256}$.  Therefore, '\n",
      "             '$\\\\dfrac{63}{256} + 2p = 1$, giving $$ '\n",
      "             'p=\\\\frac{1}{2}\\\\left(1-\\\\frac{63}{256}\\\\right)=\\\\boxed{\\\\frac{193}{512}} '\n",
      "             '$$',\n",
      " 'type': 'Counting & Probability'}\n"
     ]
    }
   ],
   "source": [
    "#now checking how well it performs\n",
    "import json \n",
    "import pprint\n",
    "from functions import load_data\n",
    "\n",
    "test_ques_1 = load_data('test', 'counting_and_probability', 2)\n",
    "pprint.pprint(test_ques_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e5e1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation = eval(test_data)\n",
    "\n",
    "problem = test_ques_1['problem']\n",
    "solution = test_ques_1['solution']\n",
    "\n",
    "prompt = f\"Solve this math problem step by step: {problem}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22604b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem:\n",
      " We flip a fair coin 10 times.  What is the probability that we get heads in at least 6 of the 10 flips?\n",
      "Original Solution:\n",
      " There are $2^{10} = 1024$ possible outcomes of the 10 coin flips. The probability that we flip at least 6 heads is equal to the probability that we flip at least 6 tails, by symmetry.  Let's call this probability $p$.  The only other possibility is that we flip exactly 5 heads and 5 tails, for which the probability is $\\dfrac{\\binom{10}{5}}{2^{10}} = \\dfrac{252}{1024} = \\dfrac{63}{256}$.  Therefore, $\\dfrac{63}{256} + 2p = 1$, giving $$ p=\\frac{1}{2}\\left(1-\\frac{63}{256}\\right)=\\boxed{\\frac{193}{512}} $$\n",
      "Model's Solution:\n",
      " The probability that we get heads in at least 6 flips is 10 / 10 = 3 times. The probability that we get heads in at least 6 flips is 3 * 3 = 6 times. The probability that we get heads in at least 6 flips is 3 * 6 = 24 times. The probability that we get heads in at least 6 flips is 24 * 6 = 84 times. The probability that we get heads in at least 6 flips is 24 * 6 = 84 times. The probability that we get heads in at least 6 flips is 24 * 6 = 84 times. The probability that we get heads in at least 6 flips is 24 * 6 = 84 times. The probability that we get heads in at least 6 flips is 24 * 6 = 84 times. The probability that we get heads in at least 6 flips is 24 * 6 = 84 times. The probability that we get heads in at least 6 flips is\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProblem:\\n\", problem)\n",
    "print(\"Original Solution:\\n\", solution)\n",
    "print(\"Model's Solution:\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a87f9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem:\n",
      " If $e^{i \\alpha} = \\frac{3}{5}  +\\frac{4}{5} i$ and $e^{i \\beta} = -\\frac{12}{13} + \\frac{5}{13} i,$ then find $\\cos (\\alpha - \\beta).$\n",
      "Original Solution:\n",
      " Dividing the given equations, we obtain\n",
      "\\begin{align*}\n",
      "e^{i (\\alpha - \\beta)} &= \\frac{\\frac{3}{5}  +\\frac{4}{5} i}{-\\frac{12}{13} + \\frac{5}{13} i} \\\\\n",
      "&= \\frac{(\\frac{3}{5}  +\\frac{4}{5} i)(-\\frac{12}{13} - \\frac{5}{13} i)}{(-\\frac{12}{13} + \\frac{5}{13} i)(-\\frac{12}{13} - \\frac{5}{13} i)} \\\\\n",
      "&= -\\frac{16}{65} - \\frac{63}{65} i.\n",
      "\\end{align*}But $e^{i (\\alpha - \\beta)} = \\cos (\\alpha - \\beta) + i \\sin (\\alpha - \\beta),$ so $\\cos (\\alpha - \\beta) = \\boxed{-\\frac{16}{65}}.$\n",
      "Model's Solution:\n",
      " cos = 35 +frac45 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35 i$ 35 i = 35\n"
     ]
    }
   ],
   "source": [
    "#on different topic\n",
    "test_ques_2 = load_data('test','precalculus',21)\n",
    "\n",
    "problem = test_ques_2['problem']\n",
    "solution = test_ques_2['solution']\n",
    "\n",
    "prompt = f\"Solve this math problem step by step: {problem}\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nProblem:\\n\", problem)\n",
    "print(\"Original Solution:\\n\", solution)\n",
    "print(\"Model's Solution:\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7443069",
   "metadata": {},
   "source": [
    "Human-Evaluation the model's answer is not right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6760658",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
